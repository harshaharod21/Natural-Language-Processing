{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZTwdUTIopvSGC7DviRTBR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshaharod21/Natural-Language-Processing/blob/main/NLP_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started with NLP\n",
        "\n",
        "We will be using both NLTK and spaCy library to perform NLP tasks\n",
        "\n",
        "**When to use spaCy or NLTK?**\n",
        "* For small-scale projects: If you’re working on a small-scale project that\n",
        "doesn’t require complex NLP tasks, NLTK might be a good choice due to its flexibility and ease of use. You can easily customize and fine-tune NLTK to suit your needs, and its extensive documentation makes it easy to get started.\n",
        "* For large-scale projects: If you’re working with large amounts of text and need to process it quickly, spaCy is the better choice due to its speed and efficiency. Its tokenization algorithm is also more efficient, making it suitable for processing complex sentences and non-standard text formats.\n",
        "* For specialized tasks: If you need to perform more specialized NLP tasks such as sentiment analysis, text classification, or named entity recognition, spaCy might be the better choice due to its advanced features and pre-trained models.\n",
        "* For multilingual projects: If you’re working with non-English text, NLTK is the better choice as it supports a wide range of languages.\n",
        "* For research or educational purposes: NLTK is a popular choice among researchers and educators due to its flexibility, extensive documentation, and large community. It’s also a good choice if you want to build your models from scratch.\n"
      ],
      "metadata": {
        "id": "JXiz_tit-A9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing - 1"
      ],
      "metadata": {
        "id": "jxz_0_IHtW6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saaUUxJ89nqq",
        "outputId": "c40bd6a4-1b04-490d-9b34-331e2ce58251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenisation"
      ],
      "metadata": {
        "id": "mhuxnJs7HwxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\" I'm a student of Data Science.\n",
        " I study at IIT Ropar and IIM Amritsar,its a wonderful experience!\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "xt7UokLsAdf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGBHZLwCA6BB",
        "outputId": "6f43ea7c-9f8b-45f6-ca31-bee2f026d90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I'm a student of Data Science.\n",
            " I study at IIT Ropar and IIM Amritsar,its a wonderful experience!\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenisation where we will convert paragraphs into sentences\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n"
      ],
      "metadata": {
        "id": "FF1s4VHmBIAa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYBQMNQPCCF3",
        "outputId": "8fc8ae3a-ffce-4251-88ce-a3ec87b63a9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "7-qUoD-wBbJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenisation\n",
        "#Convert paragraph/sentences into words\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "jq3ZUeSQCrs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "eJlyYamBDBoN",
        "outputId": "279de492-e1bb-4703-9d8b-778bc746101f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'word_tokenize' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d865a17a4564>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "PnLoFaGiDaHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#treating punctuation as diiferent word\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYB5VHV4DqS7",
        "outputId": "5be714e4-b5b9-4885-f514-6cb175a73a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " \"'\",\n",
              " 'm',\n",
              " 'a',\n",
              " 'student',\n",
              " 'of',\n",
              " 'Data',\n",
              " 'Science',\n",
              " '.',\n",
              " 'I',\n",
              " 'study',\n",
              " 'at',\n",
              " 'IIT',\n",
              " 'Ropar',\n",
              " 'and',\n",
              " 'IIM',\n",
              " 'Amritsar',\n",
              " ',',\n",
              " 'its',\n",
              " 'a',\n",
              " 'wonderful',\n",
              " 'experience',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "SORkddsDEXEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "lErp1h1YEeos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fullstop is not a seperate word\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEE9x-hGEodX",
        "outputId": "59624f8c-4afd-472b-eaec-31e9229c954c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " \"'m\",\n",
              " 'a',\n",
              " 'student',\n",
              " 'of',\n",
              " 'Data',\n",
              " 'Science.',\n",
              " 'I',\n",
              " 'study',\n",
              " 'at',\n",
              " 'IIT',\n",
              " 'Ropar',\n",
              " 'and',\n",
              " 'IIM',\n",
              " 'Amritsar',\n",
              " ',',\n",
              " 'its',\n",
              " 'a',\n",
              " 'wonderful',\n",
              " 'experience',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "9GCiMzVNH1Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## eating,eat,eaten--->eat   go,going,gone---->go"
      ],
      "metadata": {
        "id": "PgCEVf4FHuHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=['go','goes','gone','goa','write','writing','written']\n"
      ],
      "metadata": {
        "id": "IE6Td79TIyvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PorterStemmer"
      ],
      "metadata": {
        "id": "dYCrZjocJA4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "Xs3XJOA_JGIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming=PorterStemmer()\n",
        "for word in words:\n",
        "  print(word+\"--->\"+stemming.stem(word))\n",
        "\n",
        "#disadvantage can be seen below"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6iN3uZRJP9F",
        "outputId": "fd70fc9a-356b-444c-aed7-41f6d7629c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go--->go\n",
            "goes--->goe\n",
            "gone--->gone\n",
            "goa--->goa\n",
            "write--->write\n",
            "writing--->write\n",
            "written--->written\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RegexpStemmer class"
      ],
      "metadata": {
        "id": "e2rQE6QcKOAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "o2ZVaEioJ7Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stem=RegexpStemmer('ing$|s$|e$|able$',min=4)"
      ],
      "metadata": {
        "id": "mRxmG8mQKYuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stem.stem('flys')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y_PNA05xLFXc",
        "outputId": "0c2e81ab-a87e-4424-c111-76b7f2f61cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Snowball Stemmer"
      ],
      "metadata": {
        "id": "eeTK_6GCLaA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "h5elcq-vLecu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#supports a number of languages,somewhat better than porterstemmer\n",
        "\n",
        "snow_stem=SnowballStemmer('english')\n",
        "\n",
        "##Major disadvantage of stemming can be observed"
      ],
      "metadata": {
        "id": "QpF4jyaMLsUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "It reduces word to its **root word** and not root stem by having access to dictionary\n",
        "\n",
        "This method takes relatively more time"
      ],
      "metadata": {
        "id": "fCFe-GYYMuHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#useful for q&a,chatbot,text summarisatiion\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "ne_xfWW8L57T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leLF0ivtN2ow",
        "outputId": "27e7837a-db52-4506-aeef-d955ca3733d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "i-eyGiNkNleZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#POS-Noun:n,verb:v,adjective:a,adverd:r\n",
        "lem.lemmatize('better',pos='a')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SRoafxbFNpzl",
        "outputId": "0e4b1311-2472-4743-e22e-67a7ea470863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"--->\"+lem.lemmatize(word,pos='v')) #by default pos='n'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO0H0GVvOuhC",
        "outputId": "4abd9901-2478-451f-c537-ded44d344934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go--->go\n",
            "goes--->go\n",
            "gone--->go\n",
            "goa--->goa\n",
            "write--->write\n",
            "writing--->write\n",
            "written--->write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop Words\n",
        "\n",
        "These are those words which are repetetive and not of much importance"
      ],
      "metadata": {
        "id": "aT9cwuOMt7Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "1ksaQyiCuJd3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQaqEQcEuaqF",
        "outputId": "8b3ef0ac-27fd-4609-b570-73e7890459f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')\n",
        "#it is good to create your own stopwords"
      ],
      "metadata": {
        "id": "oGsTbuUquiqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n"
      ],
      "metadata": {
        "id": "DtTVnBZXya8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para=\"\"\"So far, we have explored two different families of generative models that have both involved latent variables—variational autoencoders (VAEs) and generative adversarial networks (GANs). In both cases, a new variable is introduced with a distribution that is easy to sample from and the model learns how to decode this variable back into the original domain.\n",
        "\n",
        "We will now turn our attention to autoregressive models—a family of models that simplify the generative modeling problem by treating it as a sequential process. Autoregressive models condition predictions on previous values in the sequence, rather than on a latent random variable. Therefore, they attempt to explicitly model the data-generating distribution rather than an approximation of it (as in the case of VAEs).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sDDBDP_szbql"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(para)"
      ],
      "metadata": {
        "id": "3VisG_flzblL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply stopwords and filter and then apply stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english')) ]\n",
        "  sentences[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "6Jhe6f_Nzqsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z2WVq1x1J8M",
        "outputId": "63afeb82-b0af-4faa-e39b-ad13ec84b020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['so far , explor two differ famili gener model involv latent variables—vari autoencod ( vae ) gener adversari network ( gan ) .',\n",
              " 'in case , new variabl introduc distribut easi sampl model learn decod variabl back origin domain .',\n",
              " 'we turn attent autoregress models—a famili model simplifi gener model problem treat sequenti process .',\n",
              " 'autoregress model condit predict previou valu sequenc , rather latent random variabl .',\n",
              " 'therefor , attempt explicitli model data-gener distribut rather approxim ( case vae ) .']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#snowball making sure that all the letter become small"
      ],
      "metadata": {
        "id": "P5X0zIFwK4c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using Lemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lema=WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "w_c0HbjjL3CU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#better than stemming and snowball stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[lema.lemmatize(word.lower()) for word in words if word not in set(stopwords.words('english')) ]\n",
        "  sentences[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "C_ejW0ZEMU4q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences\n",
        "# sentences are not in lowercase it can be changeed to lowercase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQCfIMDINRfV",
        "outputId": "504b2814-7e03-4dcc-a1a4-bf669e8479ab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['so far , explore two different family generative model involve latent variables—variational autoencoders ( vaes ) generative adversarial network ( gans ) .',\n",
              " 'in case , new variable introduce distribution easy sample model learn decode variable back original domain .',\n",
              " 'we turn attention autoregressive models—a family model simplify generative model problem treat sequential process .',\n",
              " 'autoregressive model condition prediction previous value sequence , rather latent random variable .',\n",
              " 'therefore , attempt explicitly model data-generating distribution rather approximation ( case vaes ) .']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parts of Speech Tagging\n",
        "We have used pos='n','v' or 'a'.This will help automatically categorize in different pos"
      ],
      "metadata": {
        "id": "878Mm8KNOa3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIwd8Sz0RSRZ",
        "outputId": "bf4fd938-7d7c-4876-98c7-ec26c5dd239f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\" Are they right? Some of these benefits are probably overhyped. Claims that a new era of personalised AI care for individual animals is just around the corner should certainly be viewed with scepticism. On broiler farms, which farm chickens for meat, chickens are slaughtered by six weeks, whereas turkeys and pigs are usually killed by the age of five or six months. It is hard to imagine individualised AI-assisted care taking off in industries in which the individuals are so quickly replaced, and even harder to envisage this in fish farming. AI products in these industries will monitor large groups, tracking averages. In the dairy and beef industries, in which animals are raised or kept for several years, providing tailored care to individuals may be more plausible.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cVoP4t0GNRYs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_a=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "ye-yfR5qPqqz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets find the Pos Tag\n",
        "\n",
        "for i in range(len(sent_a)):\n",
        "  words= nltk.word_tokenize(sent_a[i])\n",
        "  words=[word for word in words if word not in set(stopwords.words('english')) ]\n",
        "  poss_tag=nltk.pos_tag(words)\n",
        "  print(poss_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FvFAeAlP2xA",
        "outputId": "5ada266f-3703-40cb-a4d5-8db41a83ed1b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Are', 'NNP'), ('right', 'RB'), ('?', '.')]\n",
            "[('Some', 'DT'), ('benefits', 'NNS'), ('probably', 'RB'), ('overhyped', 'VBD'), ('.', '.')]\n",
            "[('Claims', 'NNP'), ('new', 'JJ'), ('era', 'NN'), ('personalised', 'VBN'), ('AI', 'NNP'), ('care', 'NN'), ('individual', 'JJ'), ('animals', 'NNS'), ('around', 'IN'), ('corner', 'NN'), ('certainly', 'RB'), ('viewed', 'VBD'), ('scepticism', 'NN'), ('.', '.')]\n",
            "[('On', 'IN'), ('broiler', 'NN'), ('farms', 'NNS'), (',', ','), ('farm', 'NN'), ('chickens', 'NNS'), ('meat', 'NN'), (',', ','), ('chickens', 'NNS'), ('slaughtered', 'VBD'), ('six', 'CD'), ('weeks', 'NNS'), (',', ','), ('whereas', 'NNS'), ('turkeys', 'VBP'), ('pigs', 'NNS'), ('usually', 'RB'), ('killed', 'VBN'), ('age', 'NN'), ('five', 'CD'), ('six', 'CD'), ('months', 'NNS'), ('.', '.')]\n",
            "[('It', 'PRP'), ('hard', 'JJ'), ('imagine', 'NN'), ('individualised', 'VBD'), ('AI-assisted', 'NNP'), ('care', 'NN'), ('taking', 'VBG'), ('industries', 'NNS'), ('individuals', 'NNS'), ('quickly', 'RB'), ('replaced', 'VBD'), (',', ','), ('even', 'RB'), ('harder', 'JJR'), ('envisage', 'NN'), ('fish', 'JJ'), ('farming', 'NN'), ('.', '.')]\n",
            "[('AI', 'NNP'), ('products', 'NNS'), ('industries', 'NNS'), ('monitor', 'VBP'), ('large', 'JJ'), ('groups', 'NNS'), (',', ','), ('tracking', 'VBG'), ('averages', 'NNS'), ('.', '.')]\n",
            "[('In', 'IN'), ('dairy', 'NN'), ('beef', 'NN'), ('industries', 'NNS'), (',', ','), ('animals', 'NNS'), ('raised', 'VBD'), ('kept', 'IN'), ('several', 'JJ'), ('years', 'NNS'), (',', ','), ('providing', 'VBG'), ('tailored', 'VBN'), ('care', 'JJ'), ('individuals', 'NNS'), ('may', 'MD'), ('plausible', 'VB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition\n",
        "To get Named entity tags:\n",
        "\n",
        "* Person: Stephen\n",
        "* Place or Location: India\n",
        "* Date: 1867\n",
        "* Time: 4:50pm\n",
        "* Money: 1 million dollar\n",
        "* Organisation: LFI Private Limited\n",
        "\n",
        "NER is a two steps process, we first perform Part of Speech (POS) tagging on the text, and then using it we extract the named entities based on the information of POS tagging\n",
        "\n",
        "**Methods in NER:**\n",
        "\n",
        "\n",
        "*   Dictionary based: generally not used\n",
        "*   Rule based\n",
        "*   Machine Learning based: It is a statistical-based model that tries to make a feature-based representation of the observed data. It can recognize an existing entity name even with small spelling variations.\n",
        "*   Deep Learning based: Deep learning NER is more accurate than the ML-based method because it is capable of assembling words, enabling it to understand the semantic and syntactic relationship between various words better\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G_OQMav5U5CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt4zWSFbWuqi",
        "outputId": "daba10e8-17a4-451c-e3ad-12e1e2bf426a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JswnOZ6QXAAZ",
        "outputId": "e28c1942-8243-40e9-99d5-f9c118725873"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVKUON7bXaK0",
        "outputId": "753237e6-a4de-478e-87f9-897235609074"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.4.0-py3-none-any.whl (23 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.4.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "n3vJast9dl45"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_large_ner(document):\n",
        "  return {(ent.text.strip(), ent.label_) for ent in NER(document).ents}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXD67T8xdoUk",
        "outputId": "380c3b24-0793-430a-9e87-746cfffc0faf"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sent_1=\"Solar eclipses have played a powerful role in human history: They have inspired numerous religious myths, and in at least one case, according to the ancient historian Herodotus and Saaleman , even stopped a war. Its about 1677 to 1891.The World Health Organization.\""
      ],
      "metadata": {
        "id": "3ZcnZAb4Vuih"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=nltk.word_tokenize(Sent_1)"
      ],
      "metadata": {
        "id": "pgegiLEYWE3n"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_elements=nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "3lGC7V9IWOL8"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.ne_chunk(tag_elements)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "cTaIhZ38WajK",
        "outputId": "bc42ffc4-fcdc-4d63-d0f3-3de9fc75254f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('GPE', [('Solar', 'JJ')]), ('eclipses', 'NNS'), ('have', 'VBP'), ('played', 'VBN'), ('a', 'DT'), ('powerful', 'JJ'), ('role', 'NN'), ('in', 'IN'), ('human', 'JJ'), ('history', 'NN'), (':', ':'), ('They', 'PRP'), ('have', 'VBP'), ('inspired', 'VBN'), ('numerous', 'JJ'), ('religious', 'JJ'), ('myths', 'NNS'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('at', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('case', 'NN'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('ancient', 'JJ'), ('historian', 'JJ'), ('Herodotus', 'NNP'), ('and', 'CC'), Tree('PERSON', [('Saaleman', 'NNP')]), (',', ','), ('even', 'RB'), ('stopped', 'VBD'), ('a', 'DT'), ('war', 'NN'), ('.', '.'), ('Its', 'PRP$'), ('about', 'IN'), ('1677', 'CD'), ('to', 'TO'), ('1891.The', 'CD'), ('World', 'NNP'), ('Health', 'NNP'), ('Organization', 'NNP'), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,2576.0,168.0\" width=\"2576px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"2.17391%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Solar</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.08696%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.10559%\" x=\"2.17391%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">eclipses</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.72671%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.86335%\" x=\"5.2795%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">have</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.21118%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.48447%\" x=\"7.14286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">played</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.38509%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.24224%\" x=\"9.62733%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.2484%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.10559%\" x=\"10.8696%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">powerful</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.4224%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.86335%\" x=\"13.9752%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">role</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.9068%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.24224%\" x=\"15.8385%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.4596%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.17391%\" x=\"17.0807%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">human</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.1677%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.79503%\" x=\"19.2547%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">history</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.6522%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"0.931677%\" x=\"22.0497%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.5155%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.86335%\" x=\"22.9814%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">They</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.913%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.86335%\" x=\"24.8447%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">have</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.7764%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.10559%\" x=\"26.7081%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">inspired</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.2609%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.10559%\" x=\"29.8137%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">numerous</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.3665%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.41615%\" x=\"32.9193%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">religious</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.6273%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.17391%\" x=\"36.3354%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">myths</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.4224%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"0.931677%\" x=\"38.5093%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.9752%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.5528%\" x=\"39.441%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.2174%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.24224%\" x=\"40.9938%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.6149%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.24224%\" x=\"42.236%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.8571%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.17391%\" x=\"43.4783%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">least</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"44.5652%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.5528%\" x=\"45.6522%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">one</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.4286%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.86335%\" x=\"47.205%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">case</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"48.1366%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"0.931677%\" x=\"49.0683%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.5342%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.41615%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">according</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.7081%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.24224%\" x=\"53.4161%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.0373%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.5528%\" x=\"54.6584%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.4348%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.79503%\" x=\"56.2112%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ancient</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.6087%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.41615%\" x=\"59.0062%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">historian</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.7143%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.41615%\" x=\"62.4224%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Herodotus</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.1304%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.5528%\" x=\"65.8385%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6149%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.10559%\" x=\"67.3913%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Saaleman</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.9441%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"0.931677%\" x=\"70.4969%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.9627%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.86335%\" x=\"71.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">even</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.3602%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.79503%\" x=\"73.2919%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">stopped</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.6894%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.24224%\" x=\"76.087%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.7081%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.5528%\" x=\"77.3292%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">war</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.1056%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"0.931677%\" x=\"78.882%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.3478%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.86335%\" x=\"79.8137%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Its</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.7453%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.17391%\" x=\"81.677%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">about</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.764%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.86335%\" x=\"83.8509%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1677</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.7826%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.24224%\" x=\"85.7143%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"86.3354%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.10559%\" x=\"86.9565%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1891.The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.5093%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.17391%\" x=\"90.0621%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">World</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.1491%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.48447%\" x=\"92.236%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Health</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"93.4783%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.34783%\" x=\"94.7205%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Organization</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.8944%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"0.931677%\" x=\"99.0683%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.5342%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc='''hello there! My name is Harsha Harod. I live in Pune, India. I workat Microsoft. '''"
      ],
      "metadata": {
        "id": "6BeGTCjDen2I"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_large_ner(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB7yQhCHZG12",
        "outputId": "e0d76f0d-05c6-4465-b115-17b4a931a8ff"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('Harsha Harod', 'PERSON'),\n",
              " ('India', 'GPE'),\n",
              " ('Microsoft', 'ORG'),\n",
              " ('Pune', 'GPE')}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(NER(doc),style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "t47pT_ShZefR",
        "outputId": "7b84919b-6d6f-4d0e-939d-de064946798b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">hello there! My name is \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Harsha Harod\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ". I live in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Pune\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". I workat \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Microsoft\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". </div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing - 2\n",
        "\n",
        "Converting input text to vectors"
      ],
      "metadata": {
        "id": "UTdn-hoXh6U7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) **One Hot Encoding:** Not in use\n",
        "\n",
        "\n",
        "* Finding unique vocabulary/words in the\n",
        "corpus.\n",
        "D1: The food is tasty\n",
        "\n",
        "D2: The food is served hot\n",
        "\n",
        "The food is tasty served hot\n",
        "\n",
        "1    0    0   0     0     0\n",
        "\n",
        "0 1 0 0 0 0\n",
        "\n",
        "Similarly for all the words...\n",
        "\n",
        "D1: [[1 0 0 0 0 0,0 1 0 0 0 0 , 0 0 1 0 0 0,0 0 0 1 0 0]]\n",
        "\n",
        "Why  not used?\n",
        "* The above matrix is Sparse Matrix (consisting of 0s and 1s).\n",
        "* No fixed sizeinput\n",
        "* No sematic meaningis not captured\n"
      ],
      "metadata": {
        "id": "kjMZN3a_i67l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Bag of words**\n",
        "\n",
        "Steps:\n",
        "\n",
        "1) Lower all the words\n",
        "\n",
        "2) Remove Stopwords\n",
        "\n",
        "\n",
        "**Vocabulary**   **Frequency**\n",
        "\n",
        "good ----------->              3\n",
        "\n",
        "boy -------------->              2\n",
        "\n",
        "girl --------------->              2\n",
        "\n",
        "good  boy  girl\n",
        "\n",
        "2  1  0  ----------> vector\n",
        "\n",
        "1  0  1\n",
        "\n",
        "1  1   1\n",
        "\n",
        "* Binary BOW and  BOW (based on frequency)\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1)Simple and intuitive\n",
        "\n",
        "2)Fixed sized I/P: good for  ML for this feature\n",
        "\n",
        "**Disadvantage:**\n",
        "\n",
        "1)Sparse matrix or array: Overfitting\n",
        "\n",
        "2)Ordering of the word is geting changed\n",
        "\n",
        "3)Out of vocabulary\n",
        "\n",
        "4)**Semantic meaning** is still not captured\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TYq-uQnjpO1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) TF-IDF** (Term Frequency - Inverse Document Frequency)\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1) Intuitive\n",
        "\n",
        "2) Fixed size, based on vocab size\n",
        "\n",
        "3) Word importance is getting captured\n",
        "\n",
        "If a word is present in all the sentences it should be given less importance,less frequent words are valued higher.\n",
        "\n",
        "**Disadvantage:**\n",
        "\n",
        "1) Sparsity is exists\n",
        "\n",
        "2) Out of vocabulary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "riZLqM789JqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embedding**:\n",
        "\n",
        "In NLP, word embedding is a term used for the representation of words for text analysis, typically in the form of real valued vector that encodes the meaning of the word such that such that the words that are closer in the vector space are expected to be similar in meaning.\n",
        "\n",
        "If lets say we have 2 vectors ( probably in higher dimension, so here we apply dimension reduction technique to reduce to two dimension)\n",
        "\n",
        "**Word Embeddings are of two types:**\n",
        "\n",
        "1) Count  or Frequency: One hot encoding, BOW, TF-IDF\n",
        "\n",
        "2) Deep Learning trained model: Word2Vec\n",
        "\n",
        "Word2Vec Architecture:\n",
        "\n",
        "* CBow\n",
        "* Skipgram\n",
        "\n",
        "\n",
        "4) **Word2Vec**: A technique by Google\n",
        "\n",
        "This algorithm uses neural network model to learn word associations from a large corpus of text. Once trained ,such model can detect synonymous words or suggest additional words for a partial sentence.\n",
        "It is **supervised learning** technique\n",
        "\n",
        "**Cosine Similarity**\n",
        "\n",
        "In this method we have two types of model\n",
        "* Pretrained Model: Trained by Google on 3 billion parameters\n",
        "* Train a model from scratch\n",
        "\n",
        "**CBOW** (Continous Bag of Words)\n",
        "\n",
        "1) Dataset: Sentences\n",
        "\n",
        "2)Window size: 5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MTiDA9lnCMZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to apply CBOW/Skipgram ?\n",
        "* Small Dataset --> CBOW\n",
        "* Huge Dataset ---> SkipGram\n",
        "\n",
        "How to improve CBOW/SkipGram?\n",
        "* Increase the training data\n",
        "* Increase the window size\n",
        "\n",
        "Feature representation of n dimension ( reduced dimension to represent the word in a vector form)\n",
        "\n",
        "**Average Word2Vec**\n",
        "\n",
        "We need to get the vector for the sentence / document and not word for classification,we get that vector D1 by taking the average value of all corresponding vector element of D1 word."
      ],
      "metadata": {
        "id": "GPZjawnQFpn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gensim Library"
      ],
      "metadata": {
        "id": "d06kaprPYuS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT_wdqUGmNUl",
        "outputId": "0221280b-9c54-4cda-a3a0-bd4b2228fd65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "XVXlUIf5ZUYZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors"
      ],
      "metadata": {
        "id": "_JIssNHUZXGM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use a google pre trained model\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "wv=api.load('word2vec-google-news-300')\n",
        "vec_king = wv['king']\n",
        "\n",
        "# it will be in 300 dimension feature representataion as vector of word 'king'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJvbouMqZjB7",
        "outputId": "e42a3961-b13d-457c-8762-970d52e7f11d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# which is the most similar word in the entire corpus\n",
        "\n",
        "wv.most_similar('teddy')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYowJEXja_eC",
        "outputId": "d1a60343-e746-45e0-eac9-c1637a9e8715"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('teddy_bear', 0.6985084414482117),\n",
              " ('cuddly_toy', 0.5974318385124207),\n",
              " ('teddybear', 0.5605696439743042),\n",
              " ('teddies', 0.545166552066803),\n",
              " ('romper_suit', 0.5435360670089722),\n",
              " ('babygro', 0.537071943283081),\n",
              " ('Pudsey_Bear', 0.5300406217575073),\n",
              " ('stuffed_animal', 0.5272604823112488),\n",
              " ('doll', 0.5216587781906128),\n",
              " ('Cuddle_Cat', 0.5186371207237244)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.similarity('tennis','sports')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e54M2AXgbev3",
        "outputId": "e53a0f64-40e1-4069-acea-97da50164ad2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4901051"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec=wv['king'] - wv['man'] + wv['women']"
      ],
      "metadata": {
        "id": "kUwx4D_0bkak"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar(vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV0oJY3TflBw",
        "outputId": "96f8e171-dee9-494d-e574-b9b3e7607af4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.6478992700576782),\n",
              " ('queen', 0.535493791103363),\n",
              " ('women', 0.5233659148216248),\n",
              " ('kings', 0.5162314772605896),\n",
              " ('queens', 0.4995364248752594),\n",
              " ('kumaris', 0.49238473176956177),\n",
              " ('princes', 0.46233269572257996),\n",
              " ('monarch', 0.45280295610427856),\n",
              " ('monarchy', 0.4293173849582672),\n",
              " ('kings_princes', 0.42342400550842285)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Word2Vec from scratch"
      ],
      "metadata": {
        "id": "-O4kHvbgkyhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converts a documnet into lowercase tokens\n",
        "\n",
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "65tzTuBglU8l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change to lowercase using the above function"
      ],
      "metadata": {
        "id": "6lA4pDUNmLui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model from scratch by playing with the parameters\n",
        "\n",
        "model= gensim.models.Word2Vec()\n"
      ],
      "metadata": {
        "id": "NAMS_RSWmPga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeGhHE0Jn4Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYJNjN09n36i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YGwb_xXjn3oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_word2vec():\n",
        "\n",
        "  return np.mean([model.wv[word] for word in doc if word in mode.wv.index_to_key], axis=0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQh6S79Jn3lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply for all sentences\n",
        "#word2vec-ML/Dl model????????\n"
      ],
      "metadata": {
        "id": "CQVpKfF2ojxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}